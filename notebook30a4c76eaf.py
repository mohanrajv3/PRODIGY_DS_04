# -*- coding: utf-8 -*-
"""notebook30a4c76eaf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook30a4c76eaf-0628d4cd-3d86-40d0-b184-4bf918511bdd.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240528/auto/storage/goog4_request%26X-Goog-Date%3D20240528T043623Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6032c8f69fcc777b03824492ea28faf8b93eb999ed8b4a95e2499c1258899b929c3be1a8c376f2dc55331712a53f0d808663d5948fc8dd6abc53e90d86e7575621730730c131888961d8564a3066dc1ee5725edec369b6c9e42d30874add0df901f2f19f3e933cb1438c2410af6c9a915db27b1d76c08557986656b94555a3a192fc79c040229989e8e718259dc4f38f10b1bdf0488401be368fac07238e8ec1450da647af3d732864f3db821d4e1d1ed7ce5516e4719d5d8a129c6affdef93e3e523085d149735878389b7ec23695e5512dbd59463831ea035d7714365098bb4de7971fb53e1b1906298f0a3f96add382aafc808c8828a6c1c4d3b56ca1d093
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'twitter-entity-sentiment-analysis:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1520310%2F2510329%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240528%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240528T043623Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D26f24e045106f849654c338eb4dbdb6a6841f9b360ee0bb1cc833953af5308a5cc3411eb20e358c7df011f8a7d3a9477d2ac6a8896e01273621fd11a2805e1ba3f60692945ca114b4448cffaa9755da1201a105bd1e9da62dc3eb30733888f96da7883c0608af8b8f1a9df165696ec969eb1345796657c5e42752459c5442218d96b287b4c70a05fa7bfd0a405b6aabce7ca114da3f9dca84ff529151afe6b60573b7dab991da212440d7075660984fcc626e88280664fd525f23f0b69457c9967aa554d71f5334a4a9d1dde58c4414f46c8eacd162028e48ae9f2f5060e1114d41a6d0fad302c72edb28cf0da1040d951c420ee47e2a0dca7fdfc10aff2607d'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
from wordcloud import WordCloud

# Load datasets
train_df = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv')
valid_df = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv')

# Print column names to identify the correct text column
print(train_df.columns)
print(valid_df.columns)

# Assuming the text column is the fourth column
text_column_index = 3
sentiment_column_index = 2

# Download NLTK data
nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
    text = str(text)  # Ensure the text is a string
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    text = re.sub(r'#', '', text)  # Remove hashtag symbol
    text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords
    return ' '.join(tokens)

# Preprocess the text column
train_df['clean_text'] = train_df.iloc[:, text_column_index].apply(preprocess_text)
valid_df['clean_text'] = valid_df.iloc[:, text_column_index].apply(preprocess_text)

print(train_df['clean_text'].head())

# Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(train_df['clean_text'])
X_valid = vectorizer.transform(valid_df['clean_text'])

y_train = train_df.iloc[:, sentiment_column_index]
y_valid = valid_df.iloc[:, sentiment_column_index]

# Sentiment Analysis Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict sentiments on the validation set
y_pred = model.predict(X_valid)

# Evaluation
print(classification_report(y_valid, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_valid, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Visualization of Sentiment Patterns
# Sentiment distribution in training data
plt.figure(figsize=(8, 6))
sns.countplot(x=y_train, palette='viridis')
plt.title('Sentiment Distribution in Training Data')
plt.show()

# Sentiment distribution in validation data
plt.figure(figsize=(8, 6))
sns.countplot(x=y_valid, palette='viridis')
plt.title('Sentiment Distribution in Validation Data')
plt.show()

# Word cloud visualization for positive sentiment
positive_text = ' '.join(train_df[train_df.iloc[:, sentiment_column_index] == 'Positive']['clean_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)

plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Positive Sentiment')
plt.show()

# Repeat the word cloud visualization for other sentiments as needed.

negative_text = ' '.join(train_df[train_df.iloc[:, sentiment_column_index] == 'Negative']['clean_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_text)

plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Negative Sentiment')
plt.show()

neutral_text = ' '.join(train_df[train_df.iloc[:, sentiment_column_index] == 'Neutral']['clean_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(neutral_text)

plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Neutral Sentiment')
plt.show()